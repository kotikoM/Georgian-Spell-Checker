{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Georgian Spellchecker: Inference Notebook\n",
    "\n",
    "Demonstration of trained CharSeq2Seq model in action, see `data_and_training.ipynb`.\n",
    "\n",
    "We will load the final weights and test the model's ability to correct various types of Georgian typos, including deletions, duplications, swaps, and random character substitutions."
   ],
   "id": "c1da854d650e1b6f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Setup\n",
    "\n",
    "First, we define the vocabulary and encoding logic used during training to ensure the input tokens match exactly."
   ],
   "id": "88df4c5614c785d0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-23T13:50:16.406846700Z",
     "start_time": "2025-12-23T13:50:16.382006400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "GEORGIAN_LETTERS = list(\"აბგდევზთიკლმნოპჟრსტუფქღყშჩცძწჭხჯჰ-\")\n",
    "special_tokens = [\"<PAD>\", \"<SOS>\", \"<EOS>\"]\n",
    "vocab = special_tokens + GEORGIAN_LETTERS\n",
    "\n",
    "char2idx = {c: i for i, c in enumerate(vocab)}\n",
    "idx2char = {i: c for i, c in enumerate(vocab)}\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "def encode_word(word):\n",
    "    return [char2idx[\"<SOS>\"]] + [char2idx[c] for c in word if c in char2idx] + [char2idx[\"<EOS>\"]]"
   ],
   "id": "948146c9c5d865b4",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Model: `correct_word()`\n",
    "I am placing correct word generation and translation inside model because why not."
   ],
   "id": "b9eb571643152611"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-23T13:50:17.698227900Z",
     "start_time": "2025-12-23T13:50:16.406846700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class CharSeq2Seq(nn.Module):\n",
    "    def __init__(self, embed_dim=64, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.encoder = nn.LSTM(embed_dim, hidden_dim, num_layers=2, batch_first=True, dropout=0.2)\n",
    "        self.decoder = nn.LSTM(embed_dim, hidden_dim, num_layers=2, batch_first=True, dropout=0.2)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def correct_word(self, word: str, max_len: int = 50) -> str:\n",
    "        self.eval()\n",
    "        device = next(self.parameters()).device\n",
    "        src = torch.tensor([encode_word(word)], device=device)\n",
    "\n",
    "        # Encoding\n",
    "        embedded_src = self.embedding(src)\n",
    "        _, hidden = self.encoder(embedded_src)\n",
    "\n",
    "        # Decoding (Greedy)\n",
    "        decoder_input = torch.tensor([[char2idx[\"<SOS>\"]]], device=device)\n",
    "        decoded_chars = []\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            embedded_input = self.embedding(decoder_input)\n",
    "            output, hidden = self.decoder(embedded_input, hidden)\n",
    "            next_token = self.fc(output).argmax(dim=-1)\n",
    "            token_id = next_token.item()\n",
    "\n",
    "            if token_id == char2idx[\"<EOS>\"]:\n",
    "                break\n",
    "            if token_id != char2idx[\"<SOS>\"] and token_id != char2idx[\"<PAD>\"]:\n",
    "                decoded_chars.append(idx2char[token_id])\n",
    "\n",
    "            decoder_input = next_token.view(1, -1)\n",
    "\n",
    "        return \"\".join(decoded_chars)"
   ],
   "id": "85a9e21ecbaaab23",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Implementation of correct_word_wrapper",
   "id": "a72d74c656417182"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-23T13:50:17.722383200Z",
     "start_time": "2025-12-23T13:50:17.699227700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def correct_word_standalone(word, model_path):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = CharSeq2Seq()\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)\n",
    "\n",
    "    return model.correct_word(word)"
   ],
   "id": "778e7eaaddb3b8e1",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Action",
   "id": "57b6aa6b1a587d5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-23T13:51:00.195449700Z",
     "start_time": "2025-12-23T13:50:59.680360200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_path = \"./trained/georgian_spellcheck_model_final.pth\"\n",
    "\n",
    "test_words = [\n",
    "    # --- Correct Words ---\n",
    "    \"გამარჯობა\", \"რობოტი\", \"გიორგი\", \"გზა\", \"მწვრთნელი\", \"იასამანი\", \"ჰიდროელექტროსადგური\",\n",
    "    # --- Deletions ---\n",
    "    \"გამარჯობ\", \"მზეზ\", \"სოკ\", \"გამ\", \"ფანჯრა\",\n",
    "    # --- Duplications ---\n",
    "    \"მთვარეე\", \"მზეზზე\", \"მგელიი\", \"წიგნნნი\", \"გოგონნა\",\n",
    "    # --- Swaps & Substitutions ---\n",
    "    \"ცამოდი\", \"დავთით\", \"ყტემალი\", \"მოტოციკკეტი\",\n",
    "    # --- Edge Cases ---\n",
    "    \"სოო\", \"\"\n",
    "]\n",
    "\n",
    "\n",
    "print(\"Spellcheck results:\\n\")\n",
    "for w in test_words:\n",
    "    corrected = correct_word_standalone(w, model_path)\n",
    "    print(f\"-    {w} -> {corrected}\")"
   ],
   "id": "9e69aaa4b9e53ad5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spellcheck results:\n",
      "\n",
      "-    გამარჯობა -> გამარჯობა\n",
      "-    რობოტი -> რობოტი\n",
      "-    გიორგი -> გიორგი\n",
      "-    გზა -> გზა\n",
      "-    მწვრთნელი -> მწვრთმელი\n",
      "-    იასამანი -> იასამანი\n",
      "-    ჰიდროელექტროსადგური -> ჰიდროელექტროსადგური\n",
      "-    გამარჯობ -> გამარჯობა\n",
      "-    მზეზ -> მზეზე\n",
      "-    სოკ -> სოკო\n",
      "-    გამ -> გამო\n",
      "-    ფანჯრა -> ფანჯარა\n",
      "-    მთვარეე -> მთვარე\n",
      "-    მზეზზე -> მზეზე\n",
      "-    მგელიი -> მგელი\n",
      "-    წიგნნნი -> წიგნი\n",
      "-    გოგონნა -> გოგონა\n",
      "-    ცამოდი -> ამოდი\n",
      "-    დავთით -> დათვით\n",
      "-    ყტემალი -> ტყემალი\n",
      "-    მოტოციკკეტი -> მოტოციკლეტი\n",
      "-    სოო -> სოლო\n",
      "-     -> ალ\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "While the model is not perfect, it successfully demonstrates how a character-level Seq2Seq architecture can master Georgian orthography. By learning the language's structural patterns, it effectively repairs common human errors.\n",
    "\n",
    "Even when faced with significant corruption, the model often recovers the intended root, proving it has moved beyond simple memorization toward a functional understanding of Georgian word formation."
   ],
   "id": "5e0a413751f52718"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
