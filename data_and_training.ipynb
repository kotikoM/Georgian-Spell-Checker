{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Georgian Spellchecker\n",
    "\n",
    "Georgian is a pretty unique language when it comes to writing: every sound maps cleanly to a single letter, there are no uppercase forms, and the alphabet itself is completely different from Latin or Cyrillic.\n",
    "\n",
    "This makes Georgian look “simple” on paper, but in practice typing errors still happen all the time: letters get duplicated, swapped, dropped, or replaced. Because Georgian spelling is so consistent, even small mistakes stand out immediately, which makes it a great candidate for a character-level spellchecking approach that learns what valid Georgian words actually look like.\n",
    "\n",
    "We tackle the problem of spellchecking Georgian words using a character-level sequence-to-sequence neural network. Unlike contextual spellcheckers that rely on surrounding text, this model operates on individual words in isolation and learns to correct spelling errors by transforming a corrupted character sequence into its correct form.\n",
    "\n",
    "By training on pairs of misspelled and correctly spelled Georgian words, the model internalizes the structural and orthographic rules of the Georgian alphabet and learns how realistic typing errors - such as missing, duplicated, swapped, or mistyped characters - can be repaired at the character level."
   ],
   "id": "f2b65f7d9fd14c61"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Getting Georgian Words\n",
    "\n",
    "Firstly, we need a large list of correctly spelled Georgian words.\n",
    "\n",
    "There are a lot of Georgian words on the internet that are already in ready-to-use format for model training purposes.\n",
    "\n",
    "Notable github repos:\n",
    "- https://github.com/gamag/ka_GE.spell/tree/master/dictionaries\n",
    "- https://github.com/akalongman/geo-words\n",
    "- https://github.com/AleksandreSukh/GeorgianWordsDataBase/blob/master/wordsChunk_2.json\n",
    "\n",
    "As well as online dictionaries:\n",
    "- http://www.nplg.gov.ge/gwdict/\n",
    "- https://www.ganmarteba.ge/\n",
    "\n",
    "For the sole purpose of having fun I chose option to crawl [ganmarteba online dictionary](https://www.ganmarteba.ge/) and as a result got about 40,000 unique words."
   ],
   "id": "a7423c08af159d46"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "I will not be including crawler code logic since it took me about 1 hour to scrape all their words, but you can take a look at implementaion in *ganmarteba.ge_crawler.py* in package *crawler*.",
   "id": "202d3640d425ce02"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-23T13:36:32.292462900Z",
     "start_time": "2025-12-23T13:36:32.179863300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "words  = open('./words/ganmarteba.ge_words.txt', 'r', encoding='utf-8').read().split('\\n')\n",
    "print(f'Collected total {len(words)} words\\n')\n",
    "\n",
    "print(f\"First couple of words:\")\n",
    "print(words[:5])"
   ],
   "id": "95be466b8a1ad755",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected total 41180 words\n",
      "\n",
      "First couple of words:\n",
      "['ააალებს', 'ააალმასებს', 'ააბარგებს', 'ააბიბინებს', 'ააბნევს']\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Corrupting Words\n",
    "\n",
    "For training, we need *source* and *target* pairs as input for our model. To simulate real-world typos, I implemented four types of error functions:\n",
    "\n",
    "- **Deletion** - randomly removes a character from the word.\n",
    "- **Duplication** - randomly repeats a character.\n",
    "- **Substitution** – replaces a character with a neighboring keyboard character.\n",
    "- **Swap** - exchanges the positions of two neighboring characters.\n",
    "\n",
    "These corruptions help the model learn how to map incorrect forms back to their correct Georgian spelling.\n",
    "\n",
    "Choosing character index for all methods is always random. Just like user behavior."
   ],
   "id": "67d2a276512eab6c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-23T13:36:32.335707200Z",
     "start_time": "2025-12-23T13:36:32.296786800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "\n",
    "def delete_char(word):\n",
    "    if len(word) <= 1:\n",
    "        return word\n",
    "    i = random.randrange(len(word))\n",
    "    return word[:i] + word[i + 1:]\n",
    "\n",
    "word = random.choice(words)\n",
    "print(f\"Result of character deletion: {word} -> {delete_char(word)}\")"
   ],
   "id": "fdf34946b5a382c5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of character deletion: დაუოკდება -> აუოკდება\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-23T13:36:32.372413300Z",
     "start_time": "2025-12-23T13:36:32.337555Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def duplicate_char(word):\n",
    "    i = random.randrange(len(word))\n",
    "    return word[:i] + word[i] + word[i:]\n",
    "\n",
    "word = random.choice(words)\n",
    "print(f\"Result of character duplication: {word} -> {duplicate_char(word)}\")"
   ],
   "id": "ea3cb2b0ddc43293",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of character duplication: აკენკვა -> აკენნკვა\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "It’s easy for a finger to accidentally hit a neighboring key on the keyboard. For this reason, we create a dictionary mapping each Georgian letter to its adjacent keys, including the shifted variants.",
   "id": "8dcfc795de213941"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-23T13:36:32.411365800Z",
     "start_time": "2025-12-23T13:36:32.373411900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "GEORGIAN_QWERTY_KEYBOARD_NEIGHBORS = {\n",
    "    \"ა\": \"ქწსზ\",\n",
    "    \"ბ\": \"ვგჰნ\",\n",
    "    \"გ\": \"ფტჰბვ\",\n",
    "    \"დ\": \"ერფცხს\",\n",
    "    \"ე\": \"წსდრ\",\n",
    "    \"ვ\": \"ცფგბ\",\n",
    "    \"ზ\": \"ასხ\",\n",
    "    \"თ\": \"ღრფგყტ\",\n",
    "    \"ი\": \"უჯკო\",\n",
    "    \"კ\": \"იოლმჯ\",\n",
    "    \"ლ\": \"კოპ\",\n",
    "    \"მ\": \"ნჯკლ\",\n",
    "    \"ნ\": \"ბჰჯმ\",\n",
    "    \"ო\": \"იკლპ\",\n",
    "    \"პ\": \"ოლ\",\n",
    "    \"ჟ\": \"ჰუიკმნჯ\",\n",
    "    \"რ\": \"ღედფგტ\",\n",
    "    \"ს\": \"აწდხზშ\",\n",
    "    \"ტ\": \"რფგჰყთ\",\n",
    "    \"უ\": \"ყჰჯკი\",\n",
    "    \"ფ\": \"დრტგვც\",\n",
    "    \"ქ\": \"წსა\",\n",
    "    \"ღ\": \"ედფგტრ\",\n",
    "    \"ყ\": \"ტგჰჯუ\",\n",
    "    \"შ\": \"აჭწდძზხს\",\n",
    "    \"ჩ\": \"ხდფვც\",\n",
    "    \"ც\": \"ხდფვჩ\",\n",
    "    \"ძ\": \"შასხძ\",\n",
    "    \"წ\": \"ქასდეჭ\",\n",
    "    \"ჭ\": \"ქაშსდეწ\",\n",
    "    \"ხ\": \"ზსდც\",\n",
    "    \"ჯ\": \"ჰუიკმნჟ\",\n",
    "    \"ჰ\": \"გყუჯნბ\",\n",
    "}"
   ],
   "id": "1e4c4412cee973b8",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-23T13:36:32.441880400Z",
     "start_time": "2025-12-23T13:36:32.412366900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def substitute_char(word):\n",
    "    i = random.randrange(len(word))\n",
    "    char = word[i]\n",
    "    neighbors = GEORGIAN_QWERTY_KEYBOARD_NEIGHBORS.get(char)\n",
    "    if not neighbors:\n",
    "        return word\n",
    "    replacement = random.choice(neighbors)\n",
    "    return word[:i] + replacement + word[i + 1:]\n",
    "\n",
    "word = random.choice(words)\n",
    "print(f\"Result of character substitution: {word} -> {substitute_char(word)}\")"
   ],
   "id": "e064999da949385e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of character substitution: თრთის -> თრტის\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-23T13:36:32.468622200Z",
     "start_time": "2025-12-23T13:36:32.443420400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def swap_chars(word):\n",
    "    if len(word) < 2:\n",
    "        return word\n",
    "    i = random.randrange(len(word) - 1)\n",
    "    return (\n",
    "            word[:i]\n",
    "            + word[i + 1]\n",
    "            + word[i]\n",
    "            + word[i + 2:]\n",
    "    )\n",
    "\n",
    "word = random.choice(words)\n",
    "print(f\"Result of character swapping: {word} -> {swap_chars(word)}\")"
   ],
   "id": "67447870b9acb944",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of character swapping: უმწეობა -> უმეწობა\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "To make the corruptions feel more natural and varied, we randomly select which functions to apply to each word.\n",
    "\n",
    "For extra control we can also choose how many errors should be applied to a word.\n",
    "\n",
    "For my model I chose:\n",
    "- if word length < 10 only 1 error\n",
    "- if word length > 10 2 or 3 error\n",
    "\n",
    "Also note that for each word we will need multiple corrupted words. I chose 4.\n",
    "\n",
    "I believe this simple approach is easier to understand than adding probability space for each function and length."
   ],
   "id": "8f5e3d16475f29df"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-23T13:36:32.484634900Z",
     "start_time": "2025-12-23T13:36:32.470841300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ERROR_FUNCTIONS = [\n",
    "    delete_char,\n",
    "    duplicate_char,\n",
    "    substitute_char,\n",
    "    swap_chars,\n",
    "]\n",
    "\n",
    "def get_corrupted_words(word, number_of_corrupted_words=4):\n",
    "    corrupted_words = set()\n",
    "\n",
    "    word_len = len(word)\n",
    "\n",
    "    # for word len < 10 1-2 error\n",
    "    # for word len > 10 3-5 error\n",
    "    word_error_count_threshold = 10\n",
    "    if word_len < word_error_count_threshold:\n",
    "        min_error, max_error = 1, 1\n",
    "    else:\n",
    "        min_error, max_error = 2, 3\n",
    "\n",
    "    while len(corrupted_words) <= number_of_corrupted_words:\n",
    "        corrupted = word\n",
    "        num_errors = random.randint(min_error, max_error)\n",
    "\n",
    "        # apply error functions\n",
    "        for _ in range(num_errors):\n",
    "            error_fn = random.choice(ERROR_FUNCTIONS)\n",
    "            corrupted = error_fn(corrupted)\n",
    "\n",
    "        corrupted_words.add(corrupted)\n",
    "\n",
    "    return list(corrupted_words)"
   ],
   "id": "bcdf78ec6a7663e5",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-23T13:36:32.525427400Z",
     "start_time": "2025-12-23T13:36:32.494359100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "    sample_words = ['თბილისი', 'ქუთაისი', 'ბათუმი', 'თელავი']\n",
    "\n",
    "    print(\"\\n__ word augmentation examples__\")\n",
    "    for w in sample_words:\n",
    "        print(f\"\\n Word: {w}\")\n",
    "        for i, c in enumerate(get_corrupted_words(w), 1):\n",
    "            print(f\"   {i:>2}. {c}\")"
   ],
   "id": "bb271a688b466813",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "__ word augmentation examples__\n",
      "\n",
      " Word: თბილისი\n",
      "    1. თბლისი\n",
      "    2. თბბილისი\n",
      "    3. თბილიისი\n",
      "    4. თბიკისი\n",
      "    5. თვილისი\n",
      "\n",
      " Word: ქუთაისი\n",
      "    1. ქთუაისი\n",
      "    2. ქუუთაისი\n",
      "    3. ქუთაიისი\n",
      "    4. ქუთასი\n",
      "    5. ქურაისი\n",
      "\n",
      " Word: ბათუმი\n",
      "    1. ბაუთმი\n",
      "    2. ბაუმი\n",
      "    3. ბათუმჯ\n",
      "    4. ბათუმ\n",
      "    5. ბათჰმი\n",
      "\n",
      " Word: თელავი\n",
      "    1. თეკავი\n",
      "    2. თელავიი\n",
      "    3. თელაი\n",
      "    4. თეავი\n",
      "    5. თეალვი\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Georgian Character Vocabulary and Encoding\n",
    "\n",
    "We define the **character vocabulary** for Georgian letters and implement helper functions to encode words into sequences of indices and decode them back.\n"
   ],
   "id": "79827279df94e2b3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-23T13:36:32.561204900Z",
     "start_time": "2025-12-23T13:36:32.536039300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Georgian alphabet and special tokens\n",
    "GEORGIAN_LETTERS = list(\"აბგდევზთიკლმნოპჟრსტუფქღყშჩცძწჭხჯჰ-\")\n",
    "special_tokens = [\"<PAD>\", \"<SOS>\", \"<EOS>\"]\n",
    "\n",
    "# Full vocabulary\n",
    "vocab = special_tokens + GEORGIAN_LETTERS\n",
    "\n",
    "# Character ↔ Index mappings\n",
    "char2idx = {c: i for i, c in enumerate(vocab)}\n",
    "idx2char = {i: c for i, c in enumerate(vocab)}\n",
    "\n",
    "# Special constants\n",
    "PAD_IDX = char2idx[\"<PAD>\"]\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(f\"Vocabulary size: {vocab_size}\")"
   ],
   "id": "338ff57ebe89a6e2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 37\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Encoding Words\n",
    "\n",
    "### The Translation Layer (Text → Numbers)\n",
    "Neural networks cannot understand the concept of the letter \"ა\" or \"ბ\". They only understand numbers (vectors).\n",
    "\n",
    "- **The Vocabulary:** This is lookup table. If \"ა\" is at index 5, then every time the model sees the number 5, it \"thinks\" of the concept of \"ა\".\n",
    "- **The Sequence:** A word becomes a list of integers. თბილისი becomes [10, 4, 11, 13, 11, 20, 11].\n",
    "\n",
    "### The \"Traffic Signals\": <SOS> and <EOS>\n",
    "\n",
    "In a Sequence-to-Sequence (Seq2Seq) model, the network needs to know when to start speaking and when to shut up. This is what these tokens handle.\n",
    "\n",
    "\n",
    "#### 1. `<SOS>` Start Of Sequence\n",
    "The Decoder part is autoregressive, meaning it generates one character at a time based on what it generated previously.\n",
    "\n",
    "- **The Problem:** To generate the first character (e.g., \"თ\"), the model needs an input to look at. But it hasn't generated anything yet!\n",
    "- **The Solution:** We feed it the `<SOS>` token.\n",
    "\n",
    "The Logic: \"If I see `<SOS>` and have the memory of the corrupted word, the first letter I should write is 'თ'.\"\n",
    "\n",
    "\n",
    "#### 2. `<EOS>` End Of Sequence\n",
    "\n",
    "Since neural networks work with fixed-size tensors, but words have variable lengths (\"მზე\" has 3 letters, \"გამარჯობა\" has 9).\n",
    "\n",
    "- **The Problem:** Without a stop signal, the model would just keep generating garbage characters until it hits the maximum allowed length (e.g., თბილისი<pad><pad><pad>...).\n",
    "\n",
    "- **The Solution:** We teach the model that after the final \"ი\", the next \"character\" is `<EOS>`.\n",
    "\n",
    "The Logic: When the model predicts `<EOS>`, our code knows the word is finished and cuts off the generation loop.\n",
    "\n",
    "#### 3. `<PAD>` Padding Token\n",
    "\n",
    "While `<SOS>` and <EOS> control the flow of a single word, `<PAD>` is essential for training multiple words at once (batching).\n",
    "\n",
    "- **The Problem:** To train efficiently, we feed the GPU a batch of words (e.g., 64 at a time). GPUs require inputs to be perfectly rectangular matrices. However, \"მზე\" (3 letters) and \"გამარჯობა\" (9 letters) have different lengths, so they cannot naturally stack into a single matrix.\n",
    "\n",
    "- **The Solution:** We extend the shorter sequences with the `<PAD>` token until they match the length of the longest word in the batch.\n",
    "\n",
    "The Logic: These tokens act as \"filler\" or silence. We explicitly tell the Loss Function to ignore PAD_IDX, ensuring the model doesn't waste effort trying to learn or predict these empty spaces."
   ],
   "id": "50110f00dd822fa9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-23T13:36:32.588571100Z",
     "start_time": "2025-12-23T13:36:32.563786600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def encode_word(word):\n",
    "    \"\"\"Encode a word as a list of character indices, including <SOS> and <EOS>.\"\"\"\n",
    "    seq = [char2idx[\"<SOS>\"]] + [char2idx[c] for c in word] + [char2idx[\"<EOS>\"]]\n",
    "    return seq\n",
    "\n",
    "word = \"თბილისი\"\n",
    "encoded = encode_word(word)\n",
    "print(f\"Original word: {word}\")\n",
    "print(f\"Encoded sequence: {encoded}\")"
   ],
   "id": "eb50d94e5c57af15",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original word: თბილისი\n",
      "Encoded sequence: [1, 10, 4, 11, 13, 11, 20, 11, 2]\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Decoding Sequences\n",
    "After the model predicts a sequence of indices, we convert them back to readable text, ignoring special tokens."
   ],
   "id": "3c7d491340d214ac"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-23T13:36:32.615832900Z",
     "start_time": "2025-12-23T13:36:32.590940900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def decode_sequence(seq):\n",
    "    \"\"\"Decode a sequence of indices into a word, skipping <SOS> and <EOS>.\"\"\"\n",
    "    chars = [idx2char[i] for i in seq if idx2char[i] not in [\"<SOS>\", \"<EOS>\"]]\n",
    "    return \"\".join(chars)\n",
    "\n",
    "decoded = decode_sequence(encoded)\n",
    "print(f\"Decoded word: {decoded}\")"
   ],
   "id": "aa56dd7709087eb6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded word: თბილისი\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### To note:\n",
    "- `vocab` contains **all Georgian letters** plug hyphen and special tokens `<PAD>`, `<SOS>`, `<EOS>`.\n",
    "- `encode_word` converts words to numeric sequences for the model.\n",
    "- `decode_sequence` converts model outputs back to text."
   ],
   "id": "6042bc6c1d57e497"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Dataset Preparation\n",
    "\n",
    "We create a **training dataset** with `(corrupted_word, correct_word)` pairs.\n",
    "Corrupted words are generated using functions for deletion, duplication, swaps, and keyboard-neighbor substitution.\n",
    "\n",
    "We shuffle the word list, generate corrupted versions, and combine them into `(input, target)` pairs.\n",
    "\n",
    "It is very **important** to include correct pairs in the input pair so model won't make arbitrary adjustments to already correct words."
   ],
   "id": "d3105af64f31a32d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-23T13:36:34.061678100Z",
     "start_time": "2025-12-23T13:36:32.617886200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "words = open('./words/ganmarteba.ge_words.txt', 'r', encoding='utf-8').read().split('\\n')\n",
    "random.shuffle(words)\n",
    "\n",
    "dataset = [\n",
    "    (encode_word(c), encode_word(w))\n",
    "    for w in words\n",
    "    for c in [w] + get_corrupted_words(w)\n",
    "]\n",
    "print(f'Generated {len(dataset)} corrupted pairs from {len(words)} words')"
   ],
   "id": "f8bd3b113cd8f002",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 247080 corrupted pairs from 41180 words\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We'll also need dataset dto",
   "id": "59fe9edf4a5952b5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-23T13:36:34.087010300Z",
     "start_time": "2025-12-23T13:36:34.064427700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SpellDataset(Dataset):\n",
    "    def __init__(self, pairs):\n",
    "        self.pairs = pairs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.pairs[idx]"
   ],
   "id": "ff1c317296df5fdf",
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## DataLoader Setup\n",
    "\n",
    "Standard neural network training requires us to feed data in \"batches\" (groups of examples) to the GPU. However, PyTorch's default batching assumes all your data inputs are identical in size (like fixed-size images).\n",
    "\n",
    "Because words vary in length (e.g., \"მზე\" is 3 letters, \"გამარჯობა\" is 9), we cannot simply stack them together. We need a custom Collate Function (collate_fn) to handle this packing process.\n",
    "\n",
    "### The Role of `collate_fn`\n",
    "\n",
    "The `collate_fn` is a helper function that runs every time the DataLoader fetches a batch of samples. Its job is to:\n",
    "\n",
    "1. Find the Maximum Length: Look at all words in the current batch and find the longest one.\n",
    "\n",
    "2. Create a Blank Canvas: Initialize a matrix (Tensor) filled entirely with our <PAD> token, sized to the longest word.\n",
    "\n",
    "3. Fill in the Data: Copy the actual word sequences into this matrix.\n"
   ],
   "id": "7a2c133151803bb3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-23T13:36:34.107236800Z",
     "start_time": "2025-12-23T13:36:34.089171900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "def collate_fn(batch):\n",
    "    src_seqs, tgt_seqs = zip(*batch)\n",
    "\n",
    "    src_lens = [len(s) for s in src_seqs]\n",
    "    tgt_lens = [len(t) for t in tgt_seqs]\n",
    "\n",
    "    max_src = max(src_lens)\n",
    "    max_tgt = max(tgt_lens)\n",
    "\n",
    "    src_batch = torch.full((len(batch), max_src), PAD_IDX, dtype=torch.long)\n",
    "    tgt_batch = torch.full((len(batch), max_tgt), PAD_IDX, dtype=torch.long)\n",
    "\n",
    "    for i, (s, t) in enumerate(zip(src_seqs, tgt_seqs)):\n",
    "        src_batch[i, :len(s)] = torch.tensor(s)\n",
    "        tgt_batch[i, :len(t)] = torch.tensor(t)\n",
    "\n",
    "    return src_batch, tgt_batch"
   ],
   "id": "d0cccf9d3cb6c424",
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We also need to split training and validation dataset. I am using 80/20 split.\n",
    "\n",
    "I am also utilizing hardware optimization of `pin_memory`.\n",
    "\n",
    "- `pin_memory=True`: This signals the CPU to prepare the data in a special area of memory (`page-locked memory`). This allows for much faster transfer of data from RAM to the GPU (CUDA).\n"
   ],
   "id": "602e8580cb1c3192"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-23T13:36:34.182328700Z",
     "start_time": "2025-12-23T13:36:34.112451300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "split_ratio = 0.8\n",
    "split_idx = int(len(dataset) * split_ratio)\n",
    "\n",
    "train_dataset = SpellDataset(dataset[:split_idx])\n",
    "val_dataset = SpellDataset(dataset[split_idx:])\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Check for GPU\n",
    "pin_memory = (device.type == \"cuda\") # If using GPU, pin_memory speeds up transfer\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=collate_fn, pin_memory=pin_memory)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn, pin_memory=pin_memory)\n",
    "\n",
    "print(f\"Training pairs: {len(train_dataset)}, Validation pairs: {len(val_dataset)}\")"
   ],
   "id": "43bdaa8bf3923d8e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training pairs: 197664, Validation pairs: 49416\n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model, Loss, Optimizer",
   "id": "f5700e463eaa2c27"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Model Architecture: The Seq2Seq Approach\n",
    "\n",
    "The architecture follows a Sequence-to-Sequence (Seq2Seq) design using Long Short-Term Memory (LSTM) networks. This choice is specifically suited for spellchecking because the model must process an entire input sequence to understand the \"context\" of a word before generating the corrected version.\n",
    "\n",
    "#### The Encoder-Decoder Structure\n",
    "The model is split into two distinct functional units:\n",
    "\n",
    "- **The Encoder:** It processes the corrupted Georgian word character by character. As it reads, it updates its internal \"hidden state.\" By the time it reaches the end of the word, this hidden state acts as a context vector—a compressed mathematical summary of the input.\n",
    "- **The Decoder:** The decoder is tasked with \"unpacking\" that context vector into a sequence of correct characters. It is autoregressive, meaning it uses the character it just predicted as the input for its next prediction.\n",
    "\n",
    "---\n",
    "#### Hyperparameter Choices\n",
    "When defining the CharSeq2Seq model, the hyperparameters are chosen to balance representational power with computational efficiency:\n",
    "\n",
    "- **Embedding Dimension** (`embed_dim=64`): Since the Georgian alphabet is relatively small (approx. 33–40 tokens), a 64-dimensional space is sufficient to capture relationships between characters. Higher dimensions risk overfitting, while lower dimensions might fail to distinguish between similar characters.\n",
    "- **Hidden Dimension** (`hidden_dim=256`): This is the size of the \"memory\" passed from the Encoder to the Decoder. 256 units provide enough workspace to store the information of long Georgian words and complex error patterns.\n",
    "- **Layers** (`num_layers=2`): Stacking two LSTMs allows the model to learn a hierarchy. The first layer handles low-level character transitions, while the second layer learns higher-level structural rules of the Georgian language.\n",
    "- **Dropout** (`0.2`): During training, 20% of the neurons are randomly deactivated. This prevents the model from \"memorizing\" specific words and forces it to learn robust spelling rules that generalize to unseen typos.\n",
    "- **Batch First** (`True`): This configuration ensures the data is processed in the format (`batch_size, sequence_length, features`), which is the standard intuitive format for modern PyTorch workflows.\n",
    "- **Teacher Forcing** (`teacher_forcing_ratio=0.5`): With a certain probability, we ignore the model's actual prediction and instead feed it the correct character from the target sequence as the input for the next step. We use a ratio of 0.5, meaning half the time the model learns from its own mistakes, and half the time it is \"guided\" by the ground truth."
   ],
   "id": "21cf6fb8346dda5d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-23T13:36:34.209483900Z",
     "start_time": "2025-12-23T13:36:34.185870300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch import nn\n",
    "\n",
    "class CharSeq2Seq(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim=64, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.encoder = nn.LSTM(embed_dim, hidden_dim, num_layers=2, batch_first=True, dropout=0.2)\n",
    "        self.decoder = nn.LSTM(embed_dim, hidden_dim, num_layers=2, batch_first=True, dropout=0.2)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt=None, teacher_forcing_ratio=0.5):\n",
    "        embedded_src = self.embedding(src)\n",
    "        _, hidden = self.encoder(embedded_src)\n",
    "\n",
    "        # decode\n",
    "        batch_size = src.size(0)\n",
    "        max_len = tgt.size(1) if tgt is not None else 30\n",
    "        outputs = torch.zeros(batch_size, max_len, vocab_size).to(src.device)\n",
    "\n",
    "        input_token = torch.full((batch_size, 1), char2idx[\"<SOS>\"], dtype=torch.long).to(src.device)\n",
    "\n",
    "        for t in range(max_len):\n",
    "            embedded_input = self.embedding(input_token)\n",
    "            output, hidden = self.decoder(embedded_input, hidden)\n",
    "            output = self.fc(output)\n",
    "\n",
    "            pred = output.argmax(2)\n",
    "            outputs[:, t, :] = output.squeeze(1)\n",
    "\n",
    "            if tgt is not None and random.random() < teacher_forcing_ratio:\n",
    "                input_token = tgt[:, t].unsqueeze(1)\n",
    "            else:\n",
    "                input_token = pred\n",
    "\n",
    "            if tgt is None:\n",
    "                if (pred == char2idx[\"<EOS>\"]).all():\n",
    "                    break\n",
    "\n",
    "        return outputs"
   ],
   "id": "e6a96209fafc1ad",
   "outputs": [],
   "execution_count": 52
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Loss Function and Optimization\n",
    "\n",
    "To turn this architecture into a functioning spellchecker, we need a way to measure its mistakes and a strategy to correct them.\n",
    "\n",
    "#### Criterion: Cross-Entropy Loss\n",
    "We use `nn.CrossEntropyLoss` as our mathematical \"judge.\" In character-level modeling, the decoder’s final layer outputs a probability distribution across the entire Georgian vocabulary for every position in the word.\n",
    "\n",
    "- **Multiclass Classification:** At its core, the model is trying to solve a classification problem for every character: \"Which of the 36+ possible Georgian characters is most likely to be here?\"\n",
    "- **The Role of** `ignore_index=PAD_IDX`: This is a crucial setting. It tells the loss function to completely ignore the positions where we added `<PAD>` tokens. We don't want to penalize the model for \"predicting\" padding, nor do we want it to waste its learning capacity on filler characters used just for batch alignment.\n",
    "\n",
    "#### Optimizer: Adam\n",
    "\n",
    "The Adam (Adaptive Moment Estimation) optimizer is used to update the model weights. Unlike standard Stochastic Gradient Descent (SGD), Adam maintains a separate learning rate for every single parameter in the model. It tracks the \"momentum\" of previous gradients, which helps the model navigate the complex loss landscape of an LSTM more efficiently."
   ],
   "id": "a62742dfe5f6f95b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-23T13:36:34.238343200Z",
     "start_time": "2025-12-23T13:36:34.211482900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "model = CharSeq2Seq().to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ],
   "id": "d13f67b79c33a21c",
   "outputs": [],
   "execution_count": 53
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Training Loop\n",
    "\n",
    "The loop is structured to handle the Autoregressive nature of the model while monitoring for Generalization (validation) versus Memorization (training).\n",
    "\n",
    "#### 1. The Training Phase & Gradient Descent\n",
    "\n",
    "We use `model.train()` to flip the internal flags for modules like `nn.Dropout`. Without this, the LSTM layers wouldn't apply the 0.2 dropout mask we defined, rendering the regularization useless.\n",
    "\n",
    "- **Zeroing Gradients:** `optimizer.zero_grad()` is a mandatory step in PyTorch because gradients accumulate by default.\n",
    "- **The Loss Objective:** We’re using a flattened Cross-Entropy Loss.\n",
    "- **Weight Update:** `loss.backward()` triggers the autograd engine, calculating the partial derivatives of the loss with respect to every weight in the Encoder and Decoder. `optimizer.step()` then adjusts those weights based on the Adam update rule.\n",
    "\n",
    "#### 2. Validation & The \"Inference Gap\"\n",
    "\n",
    "Switching to `model.eval()` and `torch.no_grad()` is standard, but the key change is setting `teacher_forcing_ratio=0.0`.\n",
    "\n",
    "- **The Reason:** During training, \"Teacher Forcing\" guides the model by feeding it the correct previous character.\n",
    "- **The Goal:** In validation, we remove the \"training wheels.\" By forcing the model to rely on its own previous predictions, we get an honest assessment of its performance in a real-world, autoregressive inference scenario.\n",
    "\n",
    "#### 3. Monitoring Metrics\n",
    "By comparing the `avg_train_loss` and `avg_val_loss`, you can monitor the bias-variance tradeoff. If the validation loss starts to diverge or rise while training loss falls, the model is memorizing the dataset rather than learning Georgian orthography."
   ],
   "id": "3b388b2aa3de8cfb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import time\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    train_loss = 0\n",
    "\n",
    "    model.train()\n",
    "    for src, tgt in train_loader:\n",
    "        # Move tensors to the same device as the model (CPU/GPU)\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "\n",
    "        # Clear accumulated gradients from the previous iteration\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass: Uses Teacher Forcing (default ratio 0.5) to stabilize early learning\n",
    "        output = model(src, tgt)\n",
    "\n",
    "        # This treats every character position as an individual classification target\n",
    "        loss = criterion(output.view(-1, model.vocab_size), tgt.view(-1))\n",
    "\n",
    "        # Backpropagation: Compute gradients for every trainable parameter\n",
    "        loss.backward()\n",
    "\n",
    "        # Update weights based on the Adam optimizer update rule\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # Calculate average training loss for the epoch\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "\n",
    "    # Set model to evaluation mode (disables Dropout for consistent inference)\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "\n",
    "    # Disable autograd engine to reduce memory overhead and speed up computation\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in val_loader:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "\n",
    "            # Disable Teacher Forcing (ratio=0.0) to evaluate true autoregressive performance\n",
    "            output = model(src, tgt, teacher_forcing_ratio=0.0)\n",
    "\n",
    "            # Use the same flattening logic to compute cross-entropy on validation data\n",
    "            loss = criterion(output.view(-1, model.vocab_size), tgt.view(-1))\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "    # 3. Logging & Performance Tracking\n",
    "    epoch_time = time.time() - start_time\n",
    "    print(\n",
    "        f\"Epoch {epoch + 1}: \"\n",
    "        f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "        f\"Val Loss: {avg_val_loss:.4f}, \"\n",
    "        f\"Time: {epoch_time:.2f}s\"\n",
    "    )"
   ],
   "id": "dcb713baf6b9e0c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Training a character-level Seq2Seq model on a vocabulary of ~40,000 words (expanded to ~200,000 pairs via augmentation) is computationally intensive.\n",
    "\n",
    "The time required for convergence depends heavily on your hardware's ability to handle the LSTM's recursive operations.\n",
    "\n",
    "It took me about 40 minutes for 10 epoch on google's T4 gpu."
   ],
   "id": "1c1467ff048bd8e1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Save Final Model\n",
    "Export the trained model to disk."
   ],
   "id": "90316212f08e073e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "final_model_path = \"georgian_spellcheck_model_final.pth\"\n",
    "torch.save(model.state_dict(), final_model_path)\n",
    "print(f\"Final model saved: {final_model_path}\")"
   ],
   "id": "62a7a14d8d197e4b"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
